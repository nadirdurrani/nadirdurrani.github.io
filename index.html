<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Nadir Durrani</title>
    <meta name="author" content="Nadir  Durrani" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>‚öõÔ∏è</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://0.0.0.0:8080/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/news/announcement_2/">news</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">research</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/spublications/">papers by area</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/competitions/">competitions</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Nadir Durrani
          </h1>
          <p class="desc"><a href="#">Arabic Language Technologies, QCRI</a></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address">
              <p>Room 1139, RDC-1</p> <p>HBKU Research Complex</p> <p>Gharrafa, Al-Rayan</p>

            </div>
          </div>

          <div class="clearfix">
            <style>
  body
  { background-image: url('assets/img/Tughra.png');
    background-repeat: no-repeat; 
    background-size: 40% 40%;
  }
</style>

<p>I am a Senior Scientist at the Arabic Language Technologies (<a href="https://alt.qcri.org/" target="_blank" rel="noopener noreferrer">ALT</a>) where I work on Interpretability (<a href="https://neurox.qcri.org/" target="_blank" rel="noopener noreferrer">NeuroX</a>), Machine Translation (<a href="https://mt.qcri.org/api" target="_blank" rel="noopener noreferrer">Shaheen</a>), Speech Synthesis (<a href="https://tts.qcri.org/" target="_blank" rel="noopener noreferrer">NatiQ</a>) and language processing tools for Arabic (<a href="https://alt.qcri.org/farasa/" target="_blank" rel="noopener noreferrer">Farasa</a>). <a href="https://www.hbku.edu.qa/en/qcri" target="_blank" rel="noopener noreferrer">QCRI</a> is a unique place that provides a flavor of academic research and productization at the same time. Please gloss through my projects and research.</p>

<p>Previously I was a Research Associate, under <a href="https://www.cs.jhu.edu/~phi/" target="_blank" rel="noopener noreferrer">Philipp Koehn</a>, at the <a href="https://web.inf.ed.ac.uk/ilcc" target="_blank" rel="noopener noreferrer">Institute of Language, Cognition and Computation</a> at the University of Edinburgh. I worked on different problems in SMT, such as Unsupervised Transliteration and Markov-based translation models.</p>

<p>Here is a periodically updated <a href="https://nadirdurrani.github.io/assets/pdf/CV.pdf" target="_blank" rel="noopener noreferrer">resume</a>.</p>

          </div>

          <!-- News -->          
          <div class="news">
            <h2>news</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <th scope="row">Aug 26, 2025</th>
                  <td>
                    üéâ Two Papers Accepted to EMNLP 2025<br> Thrilled to announce that two of our papers have been accepted to the EMNLP 2025 main conference!<br>
<a href="https://alt.qcri.org/~ndurrani/pubs/EMNLP25a.pdf" target="_blank" rel="noopener noreferrer">Editing Across Languages: A Survey of Multilingual Knowledge Editing</a><br>
<a href="https://alt.qcri.org/~ndurrani/pubs/EMNLP25b.pdf" target="_blank" rel="noopener noreferrer">Beyond the Leaderboard: Model Diffing for Understanding Performance Disparities in LLMs</a><br>
Looking forward to presenting in Suzhou this November!

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">May 19, 2025</th>
                  <td>
                    üéôÔ∏è <a href="https://alt.qcri.org/~ndurrani/pubs/INTERSPEECH25.pdf" target="_blank" rel="noopener noreferrer"> From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models </a>, has been accepted for presentation at INTERSPEECH 2025, to be held in Rotterdam, Netherlands.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Apr 20, 2025</th>
                  <td>
                    <a class="news-title" href="/news/announcement_2/">More News ...</a> 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>selected publications</h2>

            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/JMLR23.png"></div>

        <!-- Entry bib key -->
        <div id="JMLR:v24:23-0074" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Discovering Salient Neurons in deep NLP models</div>
          <!-- Author -->
          <div class="author">
          

          <em>Nadir Durrani</em>,¬†Fahim Dalvi,¬†and¬†Hassan Sajjad</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Journal of Machine Learning Research</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://jmlr.org/papers/volume24/23-0074/23-0074.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, work done towards analyzing individual neurons is relatively sparse. We present a technique called Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property, with the goal of understanding how such knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that learn a specific linguistic property? (ii) is a certain linguistic phenomenon in a given model localized (encoded in few individual neurons) or distributed across many neurons? (iii) how redundantly is the information preserved? (iv) how does fine-tuning pre-trained models towards downstream NLP tasks impact the learned linguistic knowledge? (v) how do models vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neurons that can predict different linguistic tasks; (ii) neurons capturing basic lexical information, such as suffixation, are localized in the lowermost layers; (iii) neurons learning complex concepts, such as syntactic role, are predominantly found in middle and higher layers; (iv) salient linguistic neurons are relocated from higher to lower layers during transfer learning, as the network preserves the higher layers for task-specific information; (v) we found interesting differences across pre-trained models regarding how linguistic information is preserved within them; and (vi) we found that concepts exhibit similar neuron distribution across different languages in the multilingual transformer models. Our code is publicly available as part of the NeuroX toolkit (Dalvi et al., 2023).</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">JMLR:v24:23-0074</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Discovering Salient Neurons in deep NLP models}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{362}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--40}</span><span class="p">,</span>
  <span class="na">area</span> <span class="p">=</span> <span class="s">{Neuron Analysis}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/CSL23.png"></div>

        <!-- Entry bib key -->
        <div id="sajjad2023:csl" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On the Effect of Dropping Layers of Pre-trained Transformer Models</div>
          <!-- Author -->
          <div class="author">
          

          Hassan Sajjad,¬†Fahim Dalvi,¬†<em>Nadir Durrani</em>, and
            <span class="more-authors" title="click to view 1 more author" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '1 more author' ? 'Preslav Nakov' : '1 more author';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">1 more author</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computer Speech and Language</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0885230822000596" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Transformer-based NLP models are trained using hundreds of millions or even billions of parameters, limiting their applicability in computationally constrained environments. While the number of parameters generally correlates with performance, it is not clear whether the entire network is required for a downstream task. Motivated by the recent work on pruning and distilling pre-trained models, we explore strategies to drop layers in pre-trained models, and observe the effect of pruning on downstream GLUE tasks. We were able to prune BERT, RoBERTa and XLNet models up to 40%, while maintaining up to 98% of their original performance. Additionally we show that our pruned models are on par with those built using knowledge distillation, both in terms of size and performance. Our experiments yield interesting observations such as: (i) the lower layers are most critical to maintain downstream task performance, (ii) some tasks such as paraphrase detection and sentence similarity are more robust to the dropping of layers, and (iii) models trained using different objective function exhibit different learning patterns and w.r.t the layer dropping.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sajjad2023:csl</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{London, UK, UK}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0885-2308}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.csl.2022.101429}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0885230822000596}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{January 2023}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Speech and Language}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{C}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Academic Press Ltd.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Effect of Dropping Layers of Pre-trained Transformer Models}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{77}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{101429}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">area</span> <span class="p">=</span> <span class="s">{Transfer Learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ICLR22.png"></div>

        <!-- Entry bib key -->
        <div id="dalvi2022discovering" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Discovering Latent Concepts Learned in BERT</div>
          <!-- Author -->
          <div class="author">
          

          Fahim Dalvi,¬†Abdul Rafae Khan,¬†Firoj Alam, and
            <span class="more-authors" title="click to view 3 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '3 more authors' ? 'Nadir Durrani, Jia Xu, Hassan Sajjad' : '3 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">3 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Learning Representations</em> Apr 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openreview.net/pdf?id=POTMtpYI1xH" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model‚Äôs perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. Our findings show: i) a model learns novel concepts (e.g. animal categories and demographic groups), which do not strictly adhere to any pre-defined categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and  morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered  latent concepts highlight potential biases learned in the model. We also release a novel BERT ConceptNet dataset consisting of 174 concept labels and 1M annotated instances.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dalvi2022discovering</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Discovering Latent Concepts Learned in {BERT}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dalvi, Fahim and Khan, Abdul Rafae and Alam, Firoj and Durrani, Nadir and Xu, Jia and Sajjad, Hassan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=POTMtpYI1xH}</span><span class="p">,</span>
  <span class="na">area</span> <span class="p">=</span> <span class="s">{Latent Concepts}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/CL.png"></div>

        <!-- Entry bib key -->
        <div id="belinkov-etal-2020-linguistic" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On the Linguistic Representational Power of Neural Machine Translation Models</div>
          <!-- Author -->
          <div class="author">
          

          Yonatan Belinkov,¬†<em>Nadir Durrani</em>,¬†Fahim Dalvi, and
            <span class="more-authors" title="click to view 2 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '2 more authors' ? 'Hassan Sajjad, James Glass' : '2 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">2 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Computational Linguistics</em> Apr 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://aclanthology.org/2020.cl-1.1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">belinkov-etal-2020-linguistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Linguistic Representational Power of Neural Machine Translation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computational Linguistics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cambridge, MA}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MIT Press}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.cl-1.1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/coli_a_00367}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--52}</span><span class="p">,</span>
  <span class="na">area</span> <span class="p">=</span> <span class="s">{Multilinguality}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/EACL14.png"></div>

        <!-- Entry bib key -->
        <div id="durrani-etal-2014-integrating" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Integrating an Unsupervised Transliteration Model into Statistical Machine Translation</div>
          <!-- Author -->
          <div class="author">
          

          <em>Nadir Durrani</em>,¬†Hassan Sajjad,¬†Hieu Hoang, and
            <span class="more-authors" title="click to view 1 more author" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '1 more author' ? 'Philipp Koehn' : '1 more author';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">1 more author</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2: Short Papers</em> Apr 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://aclanthology.org/E14-4029.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We investigate three methods for integrating an unsupervised transliteration model into an end-to-end SMT system. We induce a transliteration model from parallel data and use it to translate OOV words. Our approach is fully unsupervised and language independent. In the methods to integrate transliterations, we observed improvements from 0.23-0.75 (‚àÜ 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">durrani-etal-2014-integrating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Integrating an Unsupervised Transliteration Model into Statistical Machine Translation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Durrani, Nadir and Sajjad, Hassan and Hoang, Hieu and Koehn, Philipp}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Gothenburg, Sweden}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/E14-4029}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3115/v1/E14-4029}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{148--153}</span><span class="p">,</span>
  <span class="na">area</span> <span class="p">=</span> <span class="s">{Transliteration}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/ACL11.png"></div>

        <!-- Entry bib key -->
        <div id="durrani-etal-2011-joint" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Joint Sequence Translation Model with Integrated Reordering</div>
          <!-- Author -->
          <div class="author">
          

          <em>Nadir Durrani</em>,¬†Helmut Schmid,¬†and¬†Alexander Fraser</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em> Jun 2011
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://aclanthology.org/P11-1105.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the ‚ÄúN-gram‚Äù model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">durrani-etal-2011-joint</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Joint Sequence Translation Model with Integrated Reordering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Durrani, Nadir and Schmid, Helmut and Fraser, Alexander}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2011}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Portland, Oregon, USA}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/P11-1105}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1045--1054}</span><span class="p">,</span>
  <span class="na">area</span> <span class="p">=</span> <span class="s">{Translation and Reordering}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%6E%64%75%72%72%61%6E%69@%68%62%6B%75.%65%64%75.%71%61" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=K6uisFAAAAAJ&amp;hl=en" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.researchgate.net/profile/Nadir-Durrani/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a>
            <a href="https://www.linkedin.com/in/nadir-durrani-04048744" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/NadirDurrani5" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="https://dblp.org/pid/54/9012.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a>
            

            </div>

            <div class="contact-note">
              The best way to reach out to me is through email.

            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2025 Nadir  Durrani. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
